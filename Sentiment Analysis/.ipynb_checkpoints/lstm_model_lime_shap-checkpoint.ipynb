{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Build a LSTM model to perform sentiment analysis on IMDB dataset.\n",
    "Compute\n",
    "    -Performance\n",
    "    -Stability of LIME explanations\n",
    "    -Stability of SHAP explanations\n",
    "'''\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "VOCAB_SIZE = 10000\n",
    "PAD_VALUE = 0\n",
    "NUM_EPOCHS = 50\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 1e-3\n",
    "MAX_SEQ_LEN = 150\n",
    "WORD_VEC_DIMS = 50\n",
    "LSTM_UNITS = 64\n",
    "\n",
    "# initialize word_index and reverse_word_index\n",
    "word_index = {}\n",
    "reverse_word_index = {}\n",
    "# build word_index and reverse_word_index\n",
    "word_index = keras.datasets.imdb.get_word_index()\n",
    "# first few indices are reserved\n",
    "word_index = {k:(v+3) for k,v in word_index.items()} \n",
    "word_index[\"<PAD>\"] = PAD_VALUE\n",
    "word_index[\"<START>\"] = 1\n",
    "word_index[\"<UNK>\"] = 2  # unknown\n",
    "word_index[\"<UNUSED>\"] = 3\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "input_seq = tf.placeholder(tf.int32, [None, MAX_SEQ_LEN], name='input_seq')\n",
    "target_class = tf.placeholder(tf.float32, [None, 1], name='target_class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def loadData():\n",
    "    imdb = keras.datasets.imdb\n",
    "\n",
    "    (train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=VOCAB_SIZE)\n",
    "\n",
    "    train_labels = np.reshape(train_labels, (train_labels.shape[0], 1))\n",
    "    test_labels = np.reshape(test_labels, (test_labels.shape[0], 1))\n",
    "\n",
    "    valid_data = train_data[0:5000]\n",
    "    valid_labels = train_labels[0:5000]\n",
    "\n",
    "    train_data = train_data[5000:]\n",
    "    train_labels = train_labels[5000:]\n",
    "\n",
    "    return imdb, train_data, train_labels, valid_data, valid_labels, test_data, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def decodeExampleText(text, word_index, reverse_word_index):\n",
    "    '''\n",
    "    for given text, returns decoded form.\n",
    "    numbers=>words\n",
    "    '''\n",
    "    return ' '.join([reverse_word_index.get(i, '?') for i in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def preprocessData(train_data, valid_data, test_data):\n",
    "    '''\n",
    "    pad the arrays so they all have the same length,\n",
    "    then create an integer tensor of shape max_length * num_reviews.\n",
    "    we can use an embedding layer capable of handling this shape as the first layer in our network.\n",
    "    '''\n",
    "\n",
    "    train_data = keras.preprocessing.sequence.pad_sequences(train_data, value=PAD_VALUE, padding='post', maxlen=MAX_SEQ_LEN)\n",
    "    valid_data = keras.preprocessing.sequence.pad_sequences(valid_data, value=PAD_VALUE, padding='post', maxlen=MAX_SEQ_LEN)\n",
    "    test_data = keras.preprocessing.sequence.pad_sequences(test_data, value=PAD_VALUE, padding='post', maxlen=MAX_SEQ_LEN)\n",
    "    \n",
    "    train_data = np.reshape(train_data, (train_data.shape[0], MAX_SEQ_LEN))\n",
    "    valid_data = np.reshape(valid_data, (valid_data.shape[0], MAX_SEQ_LEN))\n",
    "    test_data = np.reshape(test_data, (test_data.shape[0], MAX_SEQ_LEN))\n",
    "\n",
    "    return train_data, valid_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def buildModel():\n",
    "    '''\n",
    "    returns output, cost and optimizer as tensor ops.\n",
    "    '''\n",
    "    # embedding layer\n",
    "    word_vec = tf.Variable(tf.truncated_normal([VOCAB_SIZE, WORD_VEC_DIMS]), dtype=tf.float32, name='Word-Vectors')\n",
    "    input_vec = tf.nn.embedding_lookup(word_vec, input_seq)\n",
    "\n",
    "    # rnn lstm layer\n",
    "    rnn_cell = tf.nn.rnn_cell.LSTMCell(LSTM_UNITS)\n",
    "    rnn_cell = tf.contrib.rnn.DropoutWrapper(cell=rnn_cell, output_keep_prob=0.5)\n",
    "\n",
    "    # finally, the rnn put together\n",
    "    output, _ = tf.nn.dynamic_rnn(rnn_cell, input_vec, dtype=tf.float32)\n",
    "    \n",
    "    output = tf.layers.flatten(output)\n",
    "\n",
    "    output = tf.layers.dense(output, 32)\n",
    "    output = tf.nn.relu(output)\n",
    "\n",
    "    output = tf.layers.dense(output, 1)\n",
    "    output = tf.nn.sigmoid(output)\n",
    "\n",
    "    loss = tf.losses.sigmoid_cross_entropy(target_class, output)\n",
    "   \n",
    "    optimizer = tf.train.AdamOptimizer(LEARNING_RATE).minimize(loss)\n",
    "\n",
    "    # a list of metrics to measure accuracy, precision, recall, f1-score\n",
    "    metrics = []\n",
    "\n",
    "    round_output = tf.round(output)\n",
    "    \n",
    "    accuracy = tf.metrics.accuracy(target_class, round_output, name='Accuracy')\n",
    "   \n",
    "    precision = tf.metrics.precision(target_class, round_output, name='Precision')\n",
    "  \n",
    "    recall = tf.metrics.recall(target_class, round_output, name='Recall')\n",
    "    \n",
    "    metrics.append(accuracy)\n",
    "    metrics.append(precision)\n",
    "    metrics.append(recall)\n",
    "\n",
    "    return optimizer, loss, output, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "imdb, train_x, train_y, valid_x, valid_y, test_x, test_y = loadData()\n",
    "train_x, valid_x, test_x = preprocessData(train_x, valid_x, test_x)\n",
    "\n",
    "optimizer, loss, output, metrics = buildModel()\n",
    "\n",
    "num_batches = train_x.shape[0] // BATCH_SIZE\n",
    "\n",
    "initializer_g = tf.global_variables_initializer()\n",
    "initializer_l = tf.local_variables_initializer()\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "sess.run([initializer_g, initializer_l])\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(\"Epoch {}\".format(epoch))\n",
    "\n",
    "    for batch in range(0, num_batches):\n",
    "        l = batch*BATCH_SIZE\n",
    "        r = min((batch+1)*BATCH_SIZE, train_x.shape[0]-1)\n",
    "\n",
    "        batch_x = train_x[l:r]\n",
    "        batch_y = train_y[l:r]\n",
    "\n",
    "        _, _, _, _ = sess.run([optimizer] + metrics, {input_seq: batch_x, target_class: batch_y})\n",
    "\n",
    "    # log summaries every epoch\n",
    "    acc_train, prec_train, rec_train = sess.run(metrics, {input_seq: train_x, target_class: train_y})\n",
    "    acc_valid, prec_valid, rec_valid = sess.run(metrics, {input_seq: valid_x, target_class: valid_y})\n",
    "\n",
    "    # print metrics\n",
    "    print(\"Training: Acc - {} | Prec - {} | Rec - {}\".format(acc_train[0], prec_train[0], rec_train[0]))\n",
    "    print(\"Validation: Acc - {} | Prec - {} | Rec - {}\".format(acc_valid[0], prec_valid[0], rec_valid[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Todo: \n",
    "# Calculate Performance metrics on test set (done)\n",
    "# LIME explanations (done)\n",
    "# Stability of LIME explanations on test set (done)\n",
    "# Shap explanations (done)\n",
    "# Stability of Shap explanations on test set\n",
    "# Images of LIME & Shap explanations for few random examples from test set\n",
    "# Party!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate Performance metrics on test set\n",
    "acc_test, prec_test, rec_test = sess.run(metrics, {input_seq: test_x, target_class: test_y})\n",
    "\n",
    "acc_test = acc_test[0]\n",
    "prec_test = prec_test[0]\n",
    "rec_test = rec_test[0]\n",
    "f1_score = 2.0*prec_test*rec_test/(prec_test+rec_test)\n",
    "\n",
    "# print performance metrics\n",
    "print(\"Test: Accuracy - {} | Precision - {} | Recall - {} | F1-score - {}\".format(acc_test, prec_test, rec_test, f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from lime.lime_text import LimeTextExplainer, IndexedString, TextDomainMapper\n",
    "class_names = ['negative','positive']\n",
    "explainer = LimeTextExplainer(class_names=class_names, split_expression=r'\\s+', bow=True)\n",
    "\n",
    "def makePrediction(strings):\n",
    "    '''\n",
    "    takes a list of d strings \n",
    "    and outputs a (d, k) numpy array with prediction probabilities, \n",
    "    where k is the numb/er of classes\n",
    "    '''\n",
    "    # convert d strings into shape (d,MAX_SEQ_LEN)\n",
    "    global word_index\n",
    "    for j in range(0,len(strings)):\n",
    "        strings[j] = strings[j].split(' ')\n",
    "        for i in range(0,len(strings[j])):\n",
    "            if strings[j][i] in word_index.keys():\n",
    "                strings[j][i] = word_index[strings[j][i]]\n",
    "            else:\n",
    "                strings[j][i] = word_index[\"<UNK>\"]\n",
    "    strings = np.array(strings)\n",
    "    \n",
    "    # calculate the output on strings\n",
    "    pred = sess.run(output, {input_seq: strings})\n",
    "    \n",
    "    # reshape it into (d,NUM_CLASSES) corresponding to d output probability distributions\n",
    "    pred = pred.tolist()\n",
    "    for i in range(0, len(pred)):\n",
    "        pred[i].insert(0,1-pred[i][0])\n",
    "    \n",
    "    return np.array(pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stability of explanations is calculated as follows: Stability is the ratio of change in explanation to change in input. Each explanation is a vector of values. \n",
    "Change in explanation is calculated as the magnitude of the vector obtained by taking the difference between the two explanations.\n",
    "Change in input is simply (noise/MAX_SEQ_LEN) ** 0.5\n",
    "We hope this formalization scales to LIME, SHAP and SENN well enough to be able to compare their values fairly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate Stability on test set\n",
    "# Stability = Change in explanations / Change in input\n",
    "# Average stability over the test set seems a good estimation\n",
    "import random\n",
    "%matplotlib inline\n",
    "\n",
    "def noisifyExample(example, num_samples=100, noise=5):\n",
    "    '''\n",
    "    for a given example text or array of word ids,\n",
    "    return a list of texts of size num_samples,\n",
    "    where each text is same as example except noise amount of words are removed randomly.\n",
    "    '''\n",
    "    noisy_examples = []\n",
    "    \n",
    "    if type(example) is not str: # given an array of word ids, convert it into text\n",
    "        example = [reverse_word_index[word_id] for word_id in example]\n",
    "        example = \" \".join(example)\n",
    "    \n",
    "    while len(noisy_examples) < num_samples:\n",
    "        indices_to_remove = set()\n",
    "        while len(indices_to_remove) < noise:\n",
    "            indices_to_remove.add(random.randint(0,MAX_SEQ_LEN-1))\n",
    "        new_exmp = example.split(\" \")\n",
    "        for ind in indices_to_remove:\n",
    "            new_exmp[ind] = \"<PAD>\"\n",
    "        new_exmp = \" \".join(new_exmp)\n",
    "        noisy_examples.append(new_exmp)        \n",
    "    return noisy_examples\n",
    "\n",
    "def vectorDifference(vec1, vec2):\n",
    "    '''\n",
    "    return magnitude of the vector gained by subtracting vec2 from vec1.\n",
    "    '''\n",
    "    assert len(vec1) == len(vec2)\n",
    "    n = len(vec1)\n",
    "    \n",
    "    # compute magnitude of difference between vectors\n",
    "    vec_diff = [vec1[ind]-vec2[ind] for ind in range(n)] # difference\n",
    "    vec_diff = [x**2 for x in vec_diff] # squared\n",
    "    vec_diff = sum(vec_diff)\n",
    "    vec_diff = vec_diff ** 0.5\n",
    "    \n",
    "    return vec_diff\n",
    "    \n",
    "\n",
    "def plotTwoVectors(vec1, vec2, plot_name):\n",
    "    pass\n",
    "\n",
    "def calculateStabilityLime(example, num_samples, noises):\n",
    "    '''\n",
    "    example: original text\n",
    "    num_samples: size of neighborhood around 'example'\n",
    "    noises: list of integers, each indicating the amount of noise\n",
    "    \n",
    "    returns: average stability, which is a dictionary.\n",
    "             maps noise to avg stability of 'example'.\n",
    "    \n",
    "    stability = vectorDifference(exp1,exp2) / normalised_noise ** 0.5\n",
    "    \n",
    "    change in input = normalised_noise ** 0.5\n",
    "    change in explanation = calculated using vectorDifference()\n",
    "    '''\n",
    "    assert num_samples > 0\n",
    "    \n",
    "    # get noisy_examples for example\n",
    "    noisy_examples = {}\n",
    "    for noise in noises:\n",
    "        noisy_examples[str(noise)] = noisifyExample(example, num_samples, noise)\n",
    "            \n",
    "    \n",
    "    # get explanation for example\n",
    "    exp = explainer.explain_instance(example, makePrediction, num_features=5, num_samples=2000)\n",
    "    # fig = exp.as_pyplot_figure()\n",
    "    exp = exp.as_list()\n",
    "    \n",
    "    \n",
    "    # get words from explanation\n",
    "    words = [e[0] for e in exp]\n",
    "    words = set(words)\n",
    "    \n",
    "    # get explanations for noisy_examples\n",
    "    noisy_explanations = {}\n",
    "    for noise in noises:\n",
    "        noisy_explanations[str(noise)] = []\n",
    "    \n",
    "    # calculate stability for each noisy explanation w.r.t explanation\n",
    "    stabilities = {}\n",
    "    \n",
    "    \n",
    "    for noise in noises:\n",
    "        stabilities[str(noise)] = 0\n",
    "        for noisy_example in noisy_examples[str(noise)]:\n",
    "            exp_ = exp[:]\n",
    "            noisy_exp = explainer.explain_instance(noisy_example, makePrediction, num_features=5, num_samples=2000)\n",
    "            # fig = noisy_exp.as_pyplot_figure()\n",
    "            noisy_exp = noisy_exp.as_list()\n",
    "            noisy_words = [e[0] for e in noisy_exp]\n",
    "            noisy_words = set(noisy_words)\n",
    "\n",
    "            # make exp_ and noisy_exp have the same words\n",
    "            total_words = words| noisy_words\n",
    "            \n",
    "            for word in total_words:\n",
    "                if word not in noisy_words:\n",
    "                    noisy_exp.append((word,0))\n",
    "                if word not in words:\n",
    "                    exp_.append((word,0))\n",
    "        \n",
    "            # sort both explanations by words\n",
    "            exp_.sort(key = lambda x:x[0])\n",
    "            noisy_exp.sort(key = lambda x:x[0])\n",
    "            \n",
    "            \n",
    "            exp_ = list(map(lambda x: x[1], exp_))\n",
    "            noisy_exp = list(map(lambda x: x[1], noisy_exp))\n",
    "    \n",
    "            # compute vector difference\n",
    "            vec_diff = vectorDifference(exp_, noisy_exp)\n",
    "\n",
    "            # compute stability\n",
    "            stability = vec_diff / (noise/MAX_SEQ_LEN)**0.5\n",
    "            \n",
    "            # store the value\n",
    "            stabilities[str(noise)] += stability\n",
    "        stabilities[str(noise)] /= num_samples\n",
    "    \n",
    "    return stabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# Stability of LIME explanations on test set\n",
    "# NOTE: these computations are expensive so test set will be reduced to 1000 examples\n",
    "\n",
    "# range of noises\n",
    "noises = [x for x in range(3,21,3)]\n",
    "\n",
    "# no. of noisy samples for each example in test set\n",
    "num_samples = 30\n",
    "\n",
    "# dictionary noise(key)->test_stability(value)\n",
    "test_stabilities = {}\n",
    "\n",
    "# initialize dictionary\n",
    "for noise in noises:\n",
    "    test_stabilities[str(noise)] = []\n",
    "\n",
    "# compute stability on test set over noises\n",
    "for i in range(0,len(test_samples)): # calculate stability for each of 500 test examples\n",
    "    x = test_x[i]\n",
    "    x  = decodeExampleText(x, word_index, reverse_word_index)\n",
    "    stb = calculateStabilityLime(x, num_samples, noises) # returns a dictionary key(noise) -> val(avg.stability)\n",
    "    print(stb)\n",
    "    for noise in noises:\n",
    "        test_stabilities[str(noise)].append(stb[str(noise)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "lime_stabilities = open('lime-test-stabilities.pickle', 'wb')\n",
    "pickle.dump(test_stabilities, lime_stabilities)\n",
    "lime_stabilities.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stability of Shap explanations\n",
    "# stability = change in explanation / change in input\n",
    "# change in input = (noise/MAX_SEQ_LEN) ** 0.5\n",
    "# change in explanation = magnitude of difference between explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "background_data = train_x[:100] # background data for DeepExplainer\n",
    "test_samples = test_x[:500] # calculate stability over this set\n",
    "e = shap.DeepExplainer((input_seq, output), background_data, sess)\n",
    "shap_values = e.shap_values(test_samples[:])\n",
    "shap_values = shap_values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateStabilityShap(ind, num_samples, noises):\n",
    "    '''\n",
    "    ind: index of example in test set\n",
    "    num_samples: size of neighborhood around 'example'\n",
    "    noises: list of integers, each indicating the amount of noise\n",
    "    \n",
    "    returns: average stability, which is a dictionary.\n",
    "             maps noise to avg stability of 'example'.\n",
    "    \n",
    "    stability = vectorDifference(exp1,exp2) / normalised_noise ** 0.5\n",
    "    \n",
    "    change in input = normalised_noise ** 0.5\n",
    "    change in explanation = calculated using vectorDifference()\n",
    "    '''\n",
    "    assert num_samples > 0\n",
    "    \n",
    "    # get noisy_examples for example\n",
    "    noisy_examples = {}\n",
    "    for noise in noises:\n",
    "        noisy_examples[str(noise)] = noisifyExample(test_samples[ind], num_samples, noise) # returns a list of shape num_samples x MAX_SEQ_LEN\n",
    "        for i in range(num_samples):\n",
    "            # convert text to array of word-ids\n",
    "            noisy_examples[str(noise)][i] = noisy_examples[str(noise)][i].split(\" \")\n",
    "            noisy_examples[str(noise)][i] = [word_index[word] for word in noisy_examples[str(noise)][i]]\n",
    "            noisy_examples[str(noise)][i] = np.array(noisy_examples[str(noise)][i])\n",
    "        noisy_examples[str(noise)] = np.array(noisy_examples[str(noise)])\n",
    "    \n",
    "    # get shap values for example\n",
    "    shap_values_original = shap_values[ind]\n",
    "    \n",
    "    # get shap values for noisy_examples\n",
    "    shap_values_noisy = {}\n",
    "    for noise in noises:\n",
    "        shap_values_noisy[str(noise)] = []\n",
    "        \n",
    "    # calculate stability for each noisy explanation w.r.t explanation\n",
    "    stabilities = {}\n",
    "    \n",
    "    for noise in noises:\n",
    "        stabilities[str(noise)] = 0\n",
    "        shap_values_noisy[str(noise)] = e.shap_values(noisy_examples[str(noise)][:])\n",
    "        shap_values_noisy[str(noise)] = shap_values_noisy[str(noise)][0]\n",
    "        for i in range(num_samples):\n",
    "            vector_diff = vectorDifference(shap_values_original, shap_values_noisy[str(noise)][i])\n",
    "            stability = vector_diff / (noise / MAX_SEQ_LEN) ** 0.5\n",
    "            stabilities[str(noise)] += stability\n",
    "        stabilities[str(noise)] /= num_samples            \n",
    "    return stabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# range of noises\n",
    "noises = [x for x in range(3,21,3)]\n",
    "\n",
    "# no. of noisy samples for each example in test set\n",
    "num_samples = 30\n",
    "\n",
    "# dictionary noise(key)->test_stability(value)\n",
    "test_stabilities = {}\n",
    "\n",
    "# initialize dictionary\n",
    "for noise in noises:\n",
    "    test_stabilities[str(noise)] = []\n",
    "\n",
    "# compute stability on test set over noises\n",
    "for i in range(0,len(test_samples)): # calculate stability for each of 500 test examples\n",
    "    stb = calculateStabilityLime(i, num_samples, noises) # returns a dictionary key(noise) -> val(avg.stability)\n",
    "    print(stb)\n",
    "    for noise in noises:\n",
    "        test_stabilities[str(noise)].append(stb[str(noise)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
