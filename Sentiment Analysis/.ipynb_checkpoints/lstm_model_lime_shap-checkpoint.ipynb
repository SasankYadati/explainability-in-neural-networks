{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Build a LSTM model to perform sentiment analysis on IMDB dataset.\n",
    "Compute\n",
    "    -Performance\n",
    "    -Stability of LIME explanations\n",
    "    -Stability of SHAP explanations\n",
    "'''\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "TOXIC AREA!\n",
    "GLOBAL VARIABLES AHEAD\n",
    "'''\n",
    "# hyperparameters\n",
    "VOCAB_SIZE = 10000\n",
    "PAD_VALUE = 0\n",
    "NUM_EPOCHS = 50\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 1e-3\n",
    "MAX_SEQ_LEN = 150\n",
    "WORD_VEC_DIMS = 50\n",
    "LSTM_UNITS = 64\n",
    "\n",
    "# initialize word_index and reverse_word_index\n",
    "word_index = {}\n",
    "reverse_word_index = {}\n",
    "# build word_index and reverse_word_index\n",
    "word_index = keras.datasets.imdb.get_word_index()\n",
    "# first few indices are reserved\n",
    "word_index = {k:(v+3) for k,v in word_index.items()} \n",
    "word_index[\"<PAD>\"] = PAD_VALUE\n",
    "word_index[\"<START>\"] = 1\n",
    "word_index[\"<UNK>\"] = 2  # unknown\n",
    "word_index[\"<UNUSED>\"] = 3\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "input_seq = tf.placeholder(tf.int32, [None, MAX_SEQ_LEN], name='input_seq')\n",
    "target_class = tf.placeholder(tf.float32, [None, 1], name='target_class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def loadData():\n",
    "    imdb = keras.datasets.imdb\n",
    "\n",
    "    (train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=VOCAB_SIZE)\n",
    "\n",
    "    train_labels = np.reshape(train_labels, (train_labels.shape[0], 1))\n",
    "    test_labels = np.reshape(test_labels, (test_labels.shape[0], 1))\n",
    "\n",
    "    valid_data = train_data[0:5000]\n",
    "    valid_labels = train_labels[0:5000]\n",
    "\n",
    "    train_data = train_data[5000:]\n",
    "    train_labels = train_labels[5000:]\n",
    "\n",
    "    return imdb, train_data, train_labels, valid_data, valid_labels, test_data, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def decodeExampleText(text, word_index, reverse_word_index):\n",
    "    '''\n",
    "    for given text, returns decoded form.\n",
    "    numbers=>words\n",
    "    '''\n",
    "    return ' '.join([reverse_word_index.get(i, '?') for i in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def preprocessData(train_data, valid_data, test_data):\n",
    "    '''\n",
    "    pad the arrays so they all have the same length,\n",
    "    then create an integer tensor of shape max_length * num_reviews.\n",
    "    we can use an embedding layer capable of handling this shape as the first layer in our network.\n",
    "    '''\n",
    "\n",
    "    train_data = keras.preprocessing.sequence.pad_sequences(train_data, value=PAD_VALUE, padding='post', maxlen=MAX_SEQ_LEN)\n",
    "    valid_data = keras.preprocessing.sequence.pad_sequences(valid_data, value=PAD_VALUE, padding='post', maxlen=MAX_SEQ_LEN)\n",
    "    test_data = keras.preprocessing.sequence.pad_sequences(test_data, value=PAD_VALUE, padding='post', maxlen=MAX_SEQ_LEN)\n",
    "    \n",
    "    train_data = np.reshape(train_data, (train_data.shape[0], MAX_SEQ_LEN))\n",
    "    valid_data = np.reshape(valid_data, (valid_data.shape[0], MAX_SEQ_LEN))\n",
    "    test_data = np.reshape(test_data, (test_data.shape[0], MAX_SEQ_LEN))\n",
    "\n",
    "    return train_data, valid_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def buildModel():\n",
    "    '''\n",
    "    returns output, cost and optimizer as tensor ops.\n",
    "    '''\n",
    "    # embedding layer\n",
    "    word_vec = tf.Variable(tf.truncated_normal([VOCAB_SIZE, WORD_VEC_DIMS]), dtype=tf.float32, name='Word-Vectors')\n",
    "    input_vec = tf.nn.embedding_lookup(word_vec, input_seq)\n",
    "\n",
    "    # rnn lstm layer\n",
    "    rnn_cell = tf.nn.rnn_cell.LSTMCell(LSTM_UNITS)\n",
    "    rnn_cell = tf.contrib.rnn.DropoutWrapper(cell=rnn_cell, output_keep_prob=0.5)\n",
    "\n",
    "    # finally, the rnn put together\n",
    "    output, _ = tf.nn.dynamic_rnn(rnn_cell, input_vec, dtype=tf.float32)\n",
    "    \n",
    "    output = tf.layers.flatten(output)\n",
    "\n",
    "    output = tf.layers.dense(output, 32)\n",
    "    output = tf.nn.relu(output)\n",
    "\n",
    "    output = tf.layers.dense(output, 1)\n",
    "    output = tf.nn.sigmoid(output)\n",
    "\n",
    "    loss = tf.losses.sigmoid_cross_entropy(target_class, output)\n",
    "    tf.summary.scalar('loss', loss)\n",
    "   \n",
    "    optimizer = tf.train.AdamOptimizer(LEARNING_RATE).minimize(loss)\n",
    "\n",
    "    # a list of metrics to measure accuracy, precision, recall, f1-score\n",
    "    metrics = []\n",
    "\n",
    "    round_output = tf.round(output)\n",
    "    \n",
    "    accuracy = tf.metrics.accuracy(target_class, round_output, name='Accuracy')\n",
    "    tf.summary.scalar('acc', accuracy[0])\n",
    "   \n",
    "    precision = tf.metrics.precision(target_class, round_output, name='Precision')\n",
    "  \n",
    "    recall = tf.metrics.recall(target_class, round_output, name='Recall')\n",
    "    \n",
    "    merged = tf.summary.merge_all()\n",
    "    \n",
    "    metrics.append(accuracy)\n",
    "    metrics.append(precision)\n",
    "    metrics.append(recall)\n",
    "\n",
    "    return optimizer, loss, output, metrics, merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\sai sasank y\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-7-13a214f72a50>:10: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-7-13a214f72a50>:14: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From c:\\users\\sai sasank y\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py:1259: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From <ipython-input-7-13a214f72a50>:16: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From <ipython-input-7-13a214f72a50>:18: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:From c:\\users\\sai sasank y\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\losses\\losses_impl.py:209: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From c:\\users\\sai sasank y\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From c:\\users\\sai sasank y\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\metrics_impl.py:2002: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Epoch 0\n",
      "Training: Acc - 0.6565 | Prec - 0.7641 | Rec - 0.4484\n",
      "Validation: Acc - 0.6762 | Prec - 0.8271 | Rec - 0.4418\n",
      "Epoch 1\n",
      "Training: Acc - 0.7096 | Prec - 0.8409 | Rec - 0.5151\n",
      "Validation: Acc - 0.7299 | Prec - 0.8645 | Rec - 0.5431\n",
      "Epoch 2\n",
      "Training: Acc - 0.7502 | Prec - 0.8739 | Rec - 0.5834\n",
      "Validation: Acc - 0.7601 | Prec - 0.8854 | Rec - 0.5960\n",
      "Epoch 3\n",
      "Training: Acc - 0.7738 | Prec - 0.8904 | Rec - 0.6232\n",
      "Validation: Acc - 0.7858 | Prec - 0.8976 | Rec - 0.6439\n",
      "Epoch 4\n",
      "Training: Acc - 0.7962 | Prec - 0.9001 | Rec - 0.6653\n",
      "Validation: Acc - 0.8054 | Prec - 0.9054 | Rec - 0.6808\n",
      "Epoch 5\n",
      "Training: Acc - 0.8130 | Prec - 0.9078 | Rec - 0.6959\n",
      "Validation: Acc - 0.8203 | Prec - 0.9117 | Rec - 0.7082\n",
      "Epoch 6\n",
      "Training: Acc - 0.8266 | Prec - 0.9135 | Rec - 0.7207\n",
      "Validation: Acc - 0.8330 | Prec - 0.9159 | Rec - 0.7324\n",
      "Epoch 7\n",
      "Training: Acc - 0.8381 | Prec - 0.9168 | Rec - 0.7428\n",
      "Validation: Acc - 0.8415 | Prec - 0.9199 | Rec - 0.7475\n",
      "Epoch 8\n",
      "Training: Acc - 0.8454 | Prec - 0.9214 | Rec - 0.7545\n",
      "Validation: Acc - 0.8498 | Prec - 0.9232 | Rec - 0.7624\n",
      "Epoch 9\n",
      "Training: Acc - 0.8533 | Prec - 0.9235 | Rec - 0.7697\n",
      "Validation: Acc - 0.8557 | Prec - 0.9257 | Rec - 0.7728\n",
      "Epoch 10\n",
      "Training: Acc - 0.8585 | Prec - 0.9270 | Rec - 0.7776\n",
      "Validation: Acc - 0.8619 | Prec - 0.9282 | Rec - 0.7838\n",
      "Epoch 11\n",
      "Training: Acc - 0.8647 | Prec - 0.9286 | Rec - 0.7896\n",
      "Validation: Acc - 0.8677 | Prec - 0.9299 | Rec - 0.7947\n",
      "Epoch 12\n",
      "Training: Acc - 0.8701 | Prec - 0.9304 | Rec - 0.7995\n",
      "Validation: Acc - 0.8727 | Prec - 0.9319 | Rec - 0.8035\n",
      "Epoch 13\n",
      "Training: Acc - 0.8746 | Prec - 0.9325 | Rec - 0.8071\n",
      "Validation: Acc - 0.8770 | Prec - 0.9338 | Rec - 0.8111\n",
      "Epoch 14\n",
      "Training: Acc - 0.8790 | Prec - 0.9343 | Rec - 0.8148\n",
      "Validation: Acc - 0.8810 | Prec - 0.9355 | Rec - 0.8179\n",
      "Epoch 15\n",
      "Training: Acc - 0.8827 | Prec - 0.9361 | Rec - 0.8210\n",
      "Validation: Acc - 0.8845 | Prec - 0.9363 | Rec - 0.8245\n",
      "Epoch 16\n",
      "Training: Acc - 0.8859 | Prec - 0.9362 | Rec - 0.8278\n",
      "Validation: Acc - 0.8876 | Prec - 0.9368 | Rec - 0.8308\n",
      "Epoch 17\n",
      "Training: Acc - 0.8890 | Prec - 0.9370 | Rec - 0.8335\n",
      "Validation: Acc - 0.8906 | Prec - 0.9377 | Rec - 0.8362\n",
      "Epoch 18\n",
      "Training: Acc - 0.8919 | Prec - 0.9380 | Rec - 0.8387\n",
      "Validation: Acc - 0.8934 | Prec - 0.9387 | Rec - 0.8412\n",
      "Epoch 19\n",
      "Training: Acc - 0.8945 | Prec - 0.9389 | Rec - 0.8434\n",
      "Validation: Acc - 0.8959 | Prec - 0.9397 | Rec - 0.8457\n",
      "Epoch 20\n",
      "Training: Acc - 0.8970 | Prec - 0.9400 | Rec - 0.8476\n",
      "Validation: Acc - 0.8983 | Prec - 0.9407 | Rec - 0.8498\n",
      "Epoch 21\n",
      "Training: Acc - 0.8993 | Prec - 0.9409 | Rec - 0.8517\n",
      "Validation: Acc - 0.9005 | Prec - 0.9416 | Rec - 0.8536\n",
      "Epoch 22\n",
      "Training: Acc - 0.9014 | Prec - 0.9418 | Rec - 0.8553\n",
      "Validation: Acc - 0.9025 | Prec - 0.9424 | Rec - 0.8571\n",
      "Epoch 23\n",
      "Training: Acc - 0.9033 | Prec - 0.9426 | Rec - 0.8586\n",
      "Validation: Acc - 0.9044 | Prec - 0.9431 | Rec - 0.8603\n",
      "Epoch 24\n",
      "Training: Acc - 0.9052 | Prec - 0.9432 | Rec - 0.8618\n",
      "Validation: Acc - 0.9062 | Prec - 0.9438 | Rec - 0.8634\n",
      "Epoch 25\n",
      "Training: Acc - 0.9069 | Prec - 0.9439 | Rec - 0.8647\n",
      "Validation: Acc - 0.9078 | Prec - 0.9444 | Rec - 0.8662\n",
      "Epoch 26\n",
      "Training: Acc - 0.9085 | Prec - 0.9447 | Rec - 0.8675\n",
      "Validation: Acc - 0.9094 | Prec - 0.9453 | Rec - 0.8688\n",
      "Epoch 27\n",
      "Training: Acc - 0.9100 | Prec - 0.9455 | Rec - 0.8698\n",
      "Validation: Acc - 0.9109 | Prec - 0.9460 | Rec - 0.8710\n",
      "Epoch 28\n",
      "Training: Acc - 0.9114 | Prec - 0.9462 | Rec - 0.8720\n",
      "Validation: Acc - 0.9122 | Prec - 0.9467 | Rec - 0.8733\n",
      "Epoch 29\n",
      "Training: Acc - 0.9128 | Prec - 0.9469 | Rec - 0.8742\n",
      "Validation: Acc - 0.9136 | Prec - 0.9474 | Rec - 0.8754\n",
      "Epoch 30\n",
      "Training: Acc - 0.9141 | Prec - 0.9475 | Rec - 0.8763\n",
      "Validation: Acc - 0.9148 | Prec - 0.9479 | Rec - 0.8775\n",
      "Epoch 31\n",
      "Training: Acc - 0.9153 | Prec - 0.9479 | Rec - 0.8785\n",
      "Validation: Acc - 0.9160 | Prec - 0.9484 | Rec - 0.8795\n",
      "Epoch 32\n",
      "Training: Acc - 0.9165 | Prec - 0.9485 | Rec - 0.8804\n",
      "Validation: Acc - 0.9172 | Prec - 0.9490 | Rec - 0.8814\n",
      "Epoch 33\n",
      "Training: Acc - 0.9176 | Prec - 0.9491 | Rec - 0.8822\n",
      "Validation: Acc - 0.9182 | Prec - 0.9494 | Rec - 0.8832\n",
      "Epoch 34\n",
      "Training: Acc - 0.9187 | Prec - 0.9495 | Rec - 0.8840\n",
      "Validation: Acc - 0.9193 | Prec - 0.9499 | Rec - 0.8849\n",
      "Epoch 35\n",
      "Training: Acc - 0.9197 | Prec - 0.9500 | Rec - 0.8856\n",
      "Validation: Acc - 0.9203 | Prec - 0.9504 | Rec - 0.8864\n",
      "Epoch 36\n",
      "Training: Acc - 0.9206 | Prec - 0.9506 | Rec - 0.8870\n",
      "Validation: Acc - 0.9212 | Prec - 0.9509 | Rec - 0.8879\n",
      "Epoch 37\n",
      "Training: Acc - 0.9216 | Prec - 0.9509 | Rec - 0.8887\n",
      "Validation: Acc - 0.9221 | Prec - 0.9513 | Rec - 0.8894\n",
      "Epoch 38\n",
      "Training: Acc - 0.9224 | Prec - 0.9515 | Rec - 0.8899\n",
      "Validation: Acc - 0.9230 | Prec - 0.9518 | Rec - 0.8907\n",
      "Epoch 39\n",
      "Training: Acc - 0.9233 | Prec - 0.9519 | Rec - 0.8912\n",
      "Validation: Acc - 0.9238 | Prec - 0.9523 | Rec - 0.8920\n",
      "Epoch 40\n",
      "Training: Acc - 0.9241 | Prec - 0.9523 | Rec - 0.8926\n",
      "Validation: Acc - 0.9246 | Prec - 0.9526 | Rec - 0.8934\n",
      "Epoch 41\n",
      "Training: Acc - 0.9249 | Prec - 0.9527 | Rec - 0.8939\n",
      "Validation: Acc - 0.9254 | Prec - 0.9530 | Rec - 0.8946\n",
      "Epoch 42\n",
      "Training: Acc - 0.9257 | Prec - 0.9531 | Rec - 0.8952\n",
      "Validation: Acc - 0.9260 | Prec - 0.9534 | Rec - 0.8954\n",
      "Epoch 43\n",
      "Training: Acc - 0.9262 | Prec - 0.9536 | Rec - 0.8956\n",
      "Validation: Acc - 0.9266 | Prec - 0.9539 | Rec - 0.8962\n",
      "Epoch 44\n",
      "Training: Acc - 0.9269 | Prec - 0.9540 | Rec - 0.8966\n",
      "Validation: Acc - 0.9272 | Prec - 0.9543 | Rec - 0.8971\n",
      "Epoch 45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: Acc - 0.9274 | Prec - 0.9545 | Rec - 0.8974\n",
      "Validation: Acc - 0.9279 | Prec - 0.9548 | Rec - 0.8980\n",
      "Epoch 46\n",
      "Training: Acc - 0.9281 | Prec - 0.9549 | Rec - 0.8984\n",
      "Validation: Acc - 0.9285 | Prec - 0.9552 | Rec - 0.8989\n",
      "Epoch 47\n",
      "Training: Acc - 0.9287 | Prec - 0.9553 | Rec - 0.8993\n",
      "Validation: Acc - 0.9291 | Prec - 0.9555 | Rec - 0.8997\n",
      "Epoch 48\n",
      "Training: Acc - 0.9293 | Prec - 0.9557 | Rec - 0.9000\n",
      "Validation: Acc - 0.9297 | Prec - 0.9559 | Rec - 0.9006\n",
      "Epoch 49\n",
      "Training: Acc - 0.9299 | Prec - 0.9560 | Rec - 0.9010\n",
      "Validation: Acc - 0.9299 | Prec - 0.9562 | Rec - 0.9007\n"
     ]
    }
   ],
   "source": [
    "imdb, train_x, train_y, valid_x, valid_y, test_x, test_y = loadData()\n",
    "train_x, valid_x, test_x = preprocessData(train_x, valid_x, test_x)\n",
    "\n",
    "optimizer, loss, output, metrics, merged = buildModel()\n",
    "\n",
    "num_batches = train_x.shape[0] // BATCH_SIZE\n",
    "\n",
    "initializer_g = tf.global_variables_initializer()\n",
    "initializer_l = tf.local_variables_initializer()\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "train_writer = tf.summary.FileWriter('logs/train', sess.graph)\n",
    "valid_writer = tf.summary.FileWriter('logs/valid')\n",
    "\n",
    "sess.run([initializer_g, initializer_l])\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(\"Epoch {}\".format(epoch))\n",
    "\n",
    "    for batch in range(0, num_batches):\n",
    "        l = batch*BATCH_SIZE\n",
    "        r = min((batch+1)*BATCH_SIZE, train_x.shape[0]-1)\n",
    "\n",
    "        batch_x = train_x[l:r]\n",
    "        batch_y = train_y[l:r]\n",
    "\n",
    "        _, _, _, _ = sess.run([optimizer] + metrics, {input_seq: batch_x, target_class: batch_y})\n",
    "\n",
    "    # log summaries every epoch\n",
    "    summary_train, acc_train, prec_train, rec_train = sess.run([merged]+metrics, {input_seq: train_x, target_class: train_y})\n",
    "    summary_valid, acc_valid, prec_valid, rec_valid = sess.run([merged]+metrics, {input_seq: valid_x, target_class: valid_y})\n",
    "    \n",
    "    train_writer.add_summary(summary_train, epoch)\n",
    "    valid_writer.add_summary(summary_valid, epoch)\n",
    "\n",
    "    # print metrics\n",
    "    print(\"Training: Acc - {0:.4f} | Prec - {1:.4f} | Rec - {2:.4f}\".format(acc_train[0], prec_train[0], rec_train[0]))\n",
    "    print(\"Validation: Acc - {0:.4f} | Prec - {1:.4f} | Rec - {2:.4f}\".format(acc_valid[0], prec_valid[0], rec_valid[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Todo: \n",
    "# Calculate Performance metrics on test set (done)\n",
    "# LIME explanations (done)\n",
    "# Stability of LIME explanations on test set (done)\n",
    "# Shap explanations (done)\n",
    "# Stability of Shap explanations on test set (done)\n",
    "# Images of LIME & Shap explanations for few random examples from test set (done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate Performance metrics on test set\n",
    "acc_test, prec_test, rec_test = sess.run(metrics, {input_seq: test_x, target_class: test_y})\n",
    "\n",
    "acc_test = acc_test[0]\n",
    "prec_test = prec_test[0]\n",
    "rec_test = rec_test[0]\n",
    "f1_score = 2.0*prec_test*rec_test/(prec_test+rec_test)\n",
    "\n",
    "print(\"Test Performance: Accuracy - {} | Precision - {} | Recall - {} | F1-score - {}\".format(acc_test, prec_test, rec_test, f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from lime.lime_text import LimeTextExplainer, IndexedString, TextDomainMapper\n",
    "\n",
    "class_names = ['negative','positive']\n",
    "explainer = LimeTextExplainer(class_names=class_names, split_expression=r'\\s+', bow=True)\n",
    "\n",
    "def makePrediction(strings):\n",
    "    '''\n",
    "    takes a list of d strings \n",
    "    and outputs a (d, k) numpy array with prediction probabilities, \n",
    "    where k is the numb/er of classes\n",
    "    '''\n",
    "    # convert d strings into shape (d,MAX_SEQ_LEN)\n",
    "    global word_index\n",
    "    for j in range(0,len(strings)):\n",
    "        strings[j] = strings[j].split(' ')\n",
    "        for i in range(0,len(strings[j])):\n",
    "            if strings[j][i] in word_index.keys() and word_index[strings[j][i]] < VOCAB_SIZE:\n",
    "                strings[j][i] = word_index[strings[j][i]]\n",
    "            else:\n",
    "                strings[j][i] = word_index[\"<UNK>\"]\n",
    "    strings = np.array(strings)\n",
    "    \n",
    "    # calculate the output on strings\n",
    "    pred = sess.run(output, {input_seq: strings})\n",
    "    \n",
    "    # reshape it into (d,NUM_CLASSES) corresponding to d output probability distributions\n",
    "    pred = pred.tolist()\n",
    "    for i in range(0, len(pred)):\n",
    "        pred[i].insert(0,1-pred[i][0])\n",
    "    \n",
    "    return np.array(pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stability of explanations is calculated as follows: Stability is the ratio of change in explanation to change in input. Each explanation is a vector of values. \n",
    "Change in explanation is calculated as the magnitude of the vector obtained by taking the difference between the two explanations.\n",
    "Change in input is simply (noise/MAX_SEQ_LEN) ** 0.5\n",
    "We hope this formalization scales to LIME, SHAP and SENN well enough to be able to compare their values fairly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate Stability on test set\n",
    "# Stability = Change in explanations / Change in input\n",
    "# Average stability over the test set seems a good estimation\n",
    "import random\n",
    "%matplotlib inline\n",
    "\n",
    "def noisifyExample(example, num_samples=100, noise=5):\n",
    "    '''\n",
    "    for a given example text or array of word ids,\n",
    "    return a list of texts of size num_samples,\n",
    "    where each text is same as example except noise amount of words are removed randomly.\n",
    "    '''\n",
    "    noisy_examples = []\n",
    "    \n",
    "    if type(example) is not str: # given an array of word ids, convert it into text\n",
    "        example = [reverse_word_index[word_id] for word_id in example]\n",
    "        example = \" \".join(example)\n",
    "    \n",
    "    while len(noisy_examples) < num_samples:\n",
    "        indices_to_remove = set()\n",
    "        while len(indices_to_remove) < noise:\n",
    "            indices_to_remove.add(random.randint(0,MAX_SEQ_LEN-1))\n",
    "        new_exmp = example.split(\" \")\n",
    "        for ind in indices_to_remove:\n",
    "            new_exmp[ind] = \"<PAD>\"\n",
    "        new_exmp = \" \".join(new_exmp)\n",
    "        noisy_examples.append(new_exmp)        \n",
    "    return noisy_examples\n",
    "\n",
    "def vectorDifference(vec1, vec2):\n",
    "    '''\n",
    "    return magnitude of the vector gained by subtracting vec2 from vec1.\n",
    "    '''\n",
    "    assert len(vec1) == len(vec2)\n",
    "    n = len(vec1)\n",
    "    \n",
    "    # compute magnitude of difference between vectors\n",
    "    vec_diff = [vec1[ind]-vec2[ind] for ind in range(n)] # difference\n",
    "    vec_diff = [x**2 for x in vec_diff] # squared\n",
    "    vec_diff = sum(vec_diff)\n",
    "    vec_diff = vec_diff ** 0.5\n",
    "    \n",
    "    return vec_diff\n",
    "    \n",
    "\n",
    "def calculateStabilityLime(example, num_samples, noises):\n",
    "    '''\n",
    "    example: original text\n",
    "    num_samples: size of neighborhood around 'example'\n",
    "    noises: list of integers, each indicating the amount of noise\n",
    "    \n",
    "    returns: average stability, which is a dictionary.\n",
    "             maps noise to avg stability of 'example'.\n",
    "    \n",
    "    stability = vectorDifference(exp1,exp2) / normalised_noise ** 0.5\n",
    "    \n",
    "    change in input = normalised_noise ** 0.5\n",
    "    change in explanation = calculated using vectorDifference()\n",
    "    '''\n",
    "    assert num_samples > 0\n",
    "    \n",
    "    # get noisy_examples for example\n",
    "    noisy_examples = {}\n",
    "    for noise in noises:\n",
    "        noisy_examples[str(noise)] = noisifyExample(example, num_samples, noise)\n",
    "            \n",
    "    \n",
    "    # get explanation for example\n",
    "    exp = explainer.explain_instance(example, makePrediction, num_features=5, num_samples=2000)\n",
    "    exp = exp.as_list()\n",
    "    \n",
    "    \n",
    "    # get words from explanation\n",
    "    words = [e[0] for e in exp]\n",
    "    words = set(words)\n",
    "    \n",
    "    # get explanations for noisy_examples\n",
    "    noisy_explanations = {}\n",
    "    for noise in noises:\n",
    "        noisy_explanations[str(noise)] = []\n",
    "    \n",
    "    # calculate stability for each noisy explanation w.r.t explanation\n",
    "    stabilities = {}\n",
    "    \n",
    "    \n",
    "    for noise in noises:\n",
    "        stabilities[str(noise)] = 0\n",
    "        for noisy_example in noisy_examples[str(noise)]:\n",
    "            exp_ = exp[:]\n",
    "            noisy_exp = explainer.explain_instance(noisy_example, makePrediction, num_features=5, num_samples=2000)\n",
    "            # fig = noisy_exp.as_pyplot_figure()\n",
    "            noisy_exp = noisy_exp.as_list()\n",
    "            noisy_words = [e[0] for e in noisy_exp]\n",
    "            noisy_words = set(noisy_words)\n",
    "\n",
    "            # make exp_ and noisy_exp have the same words\n",
    "            total_words = words| noisy_words\n",
    "            \n",
    "            for word in total_words:\n",
    "                if word not in noisy_words:\n",
    "                    noisy_exp.append((word,0))\n",
    "                if word not in words:\n",
    "                    exp_.append((word,0))\n",
    "        \n",
    "            # sort both explanations by words\n",
    "            exp_.sort(key = lambda x:x[0])\n",
    "            noisy_exp.sort(key = lambda x:x[0])\n",
    "            \n",
    "            \n",
    "            exp_ = list(map(lambda x: x[1], exp_))\n",
    "            noisy_exp = list(map(lambda x: x[1], noisy_exp))\n",
    "    \n",
    "            # compute vector difference\n",
    "            vec_diff = vectorDifference(exp_, noisy_exp)\n",
    "\n",
    "            # compute stability\n",
    "            stability = vec_diff / (noise/MAX_SEQ_LEN)**0.5\n",
    "            \n",
    "            # store the value\n",
    "            stabilities[str(noise)] += stability\n",
    "        stabilities[str(noise)] /= num_samples\n",
    "    \n",
    "    return stabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# Stability of LIME explanations on test set\n",
    "# NOTE: these computations are expensive so test set will be reduced to 1000 examples\n",
    "\n",
    "# range of noises\n",
    "noises = [x for x in range(3,21,3)]\n",
    "\n",
    "# no. of noisy samples for each example in test set\n",
    "num_samples = 30\n",
    "\n",
    "# dictionary noise(key)->test_stability(value)\n",
    "test_stabilities = {}\n",
    "\n",
    "# initialize dictionary\n",
    "for noise in noises:\n",
    "    test_stabilities[str(noise)] = []\n",
    "\n",
    "# compute stability on test set over noises\n",
    "for i in range(0,len(test_samples)): # calculate stability for each of 500 test examples\n",
    "    x = test_x[i]\n",
    "    x  = decodeExampleText(x, word_index, reverse_word_index)\n",
    "    stb = calculateStabilityLime(x, num_samples, noises) # returns a dictionary key(noise) -> val(avg.stability)\n",
    "    print(stb)\n",
    "    for noise in noises:\n",
    "        test_stabilities[str(noise)].append(stb[str(noise)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "lime_stabilities = open('lime-test-stabilities.pickle', 'wb')\n",
    "pickle.dump(test_stabilities, lime_stabilities)\n",
    "lime_stabilities.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stability of Shap explanations\n",
    "# stability = change in explanation / change in input\n",
    "# change in input = (noise/MAX_SEQ_LEN) ** 0.5\n",
    "# change in explanation = magnitude of difference between explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "background_data = train_x[:100] # background data for DeepExplainer\n",
    "test_samples = test_x[:500] # calculate stability over this set\n",
    "e = shap.DeepExplainer((input_seq, output), background_data, sess)\n",
    "shap_values = e.shap_values(test_samples[:])\n",
    "shap_values = shap_values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.initjs()\n",
    "feature_names = [reverse_word_index[ind] for ind in test_samples[70]]\n",
    "shap.force_plot(e.expected_value, shap_values[70], feature_names=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateStabilityShap(ind, num_samples, noises):\n",
    "    '''\n",
    "    ind: index of example in test set\n",
    "    num_samples: size of neighborhood around 'example'\n",
    "    noises: list of integers, each indicating the amount of noise\n",
    "    \n",
    "    returns: average stability, which is a dictionary.\n",
    "             maps noise to avg stability of 'example'.\n",
    "    \n",
    "    stability = vectorDifference(exp1,exp2) / normalised_noise ** 0.5\n",
    "    \n",
    "    change in input = normalised_noise ** 0.5\n",
    "    change in explanation = calculated using vectorDifference()\n",
    "    '''\n",
    "    assert num_samples > 0\n",
    "    \n",
    "    # get noisy_examples for example\n",
    "    noisy_examples = {}\n",
    "    for noise in noises:\n",
    "        noisy_examples[str(noise)] = noisifyExample(test_samples[ind], num_samples, noise) # returns a list of shape num_samples x MAX_SEQ_LEN\n",
    "        for i in range(num_samples):\n",
    "            # convert text to array of word-ids\n",
    "            noisy_examples[str(noise)][i] = noisy_examples[str(noise)][i].split(\" \")\n",
    "            noisy_examples[str(noise)][i] = [word_index[word] for word in noisy_examples[str(noise)][i]]\n",
    "            noisy_examples[str(noise)][i] = np.array(noisy_examples[str(noise)][i])\n",
    "        noisy_examples[str(noise)] = np.array(noisy_examples[str(noise)])\n",
    "    \n",
    "    # get shap values for example\n",
    "    shap_values_original = shap_values[ind]\n",
    "    \n",
    "    # get shap values for noisy_examples\n",
    "    shap_values_noisy = {}\n",
    "    for noise in noises:\n",
    "        shap_values_noisy[str(noise)] = []\n",
    "        \n",
    "    # calculate stability for each noisy explanation w.r.t explanation\n",
    "    stabilities = {}\n",
    "    \n",
    "    for noise in noises:\n",
    "        stabilities[str(noise)] = 0\n",
    "        shap_values_noisy[str(noise)] = e.shap_values(noisy_examples[str(noise)][:])\n",
    "        shap_values_noisy[str(noise)] = shap_values_noisy[str(noise)][0]\n",
    "        for i in range(num_samples):\n",
    "            vector_diff = vectorDifference(shap_values_original, shap_values_noisy[str(noise)][i])\n",
    "            stability = vector_diff / (noise / MAX_SEQ_LEN) ** 0.5\n",
    "            stabilities[str(noise)] += stability\n",
    "        stabilities[str(noise)] /= num_samples            \n",
    "    return stabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# range of noises\n",
    "noises = [x for x in range(3,21,3)]\n",
    "\n",
    "# no. of noisy samples for each example in test set\n",
    "num_samples = 30\n",
    "\n",
    "# dictionary noise(key)->test_stability(value)\n",
    "test_stabilities = {}\n",
    "\n",
    "# initialize dictionary\n",
    "for noise in noises:\n",
    "    test_stabilities[str(noise)] = []\n",
    "\n",
    "# compute stability on test set over noises\n",
    "for i in range(0,len(test_samples)): # calculate stability for each of 500 test examples\n",
    "    stb = calculateStabilityLime(i, num_samples, noises) # returns a dictionary key(noise) -> val(avg.stability)\n",
    "    print(stb)\n",
    "    for noise in noises:\n",
    "        test_stabilities[str(noise)].append(stb[str(noise)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "shap_stabilities = open('shap-test-stabilities.pickle', 'wb')\n",
    "pickle.dump(test_stabilities, shap_stabilities)\n",
    "shap_stabilities.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def showExplanationLime(text):\n",
    "    exp = explainer.explain_instance(text, makePrediction, num_features=5, num_samples=2000)\n",
    "    exp.as_pyplot_figure()\n",
    "\n",
    "def showExplanationShap(text_sample):\n",
    "    shap.initjs()\n",
    "    feature_names = [reverse_word_index[ind] for ind in test_samples[text_sample]]\n",
    "    return shap.force_plot(e.expected_value, shap_values[text_sample], feature_names=feature_names)\n",
    "\n",
    "def showExplanation(text_sample, model):\n",
    "    '''\n",
    "    for a given text, outputs the prediction using makePrediction()\n",
    "    for the given model, displays an explanation using showExplanationLime() and showExplanationShap()\n",
    "    '''\n",
    "    \n",
    "    text = test_samples[text_sample]\n",
    "    text = list(text)\n",
    "    text = \" \".join([reverse_word_index[x] for x in text])\n",
    "    print(text)\n",
    "    print(\"-------------------------------------------------------\")\n",
    "    prediction = makePrediction([text]) # returns a numpy array of shape 1x2\n",
    "    print(\"Model's prediction -> Negative: {0:0.4f}, Positive: {1:0.4f}\".format(prediction[0][0], prediction[0][1]))\n",
    "    \n",
    "    if model == \"LIME\":\n",
    "        showExplanationLime(text)\n",
    "    else:\n",
    "        return showExplanationShap(text_sample)\n",
    "    \n",
    "    return\n",
    "style = {'description_width': 'initial'}\n",
    "layout = widgets.Layout(width='100%')\n",
    "text_sample_slider = widgets.IntSlider(value=70, min=0, max=len(test_samples), description=\"Index of sample\", continuous_update=False)\n",
    "text_sample_slider.layout = layout\n",
    "text_sample_slider.style = style\n",
    "i = interact(showExplanation, text_sample=text_sample_slider, model=[\"Shap\", \"LIME\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
