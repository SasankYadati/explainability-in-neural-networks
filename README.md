# Explainability-in-neural-networks

## Abstract

When deployed for usage in the industry, machine learning models need to develop trust in the user so that he or she would accept the results of the model and use it. This would enhance faith as now the model would not be a black box. The results of the explanation based system must also be stable in the sense that there should not be a lot of change in the explanations for minor change in the input. Through this project, we wish to develop a self-explanation based architecture where the explanations are learnt through the training  process along with the actual task in hand. We demonstrate the same for the task of Image Classification performed on the MNIST dataset. We argue this architecture is extendable to other modalities as well. This model is shown to be better than the posteriori based approaches in terms of the stability and robustness of the explanations. The goal of this project is to develop the same and compare it with the posteriori based explanations. 
